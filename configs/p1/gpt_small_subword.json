{
  "vocab_size": 16384,
  "d_model": 512,
  "n_layers": 8,
  "n_heads": 8,
  "d_ff": 2048,
  "block_size": 512,
  "dropout": 0.1,

  "norm_type": "layernorm",
  "mlp_type": "gelu",
  "attention_type": "mha",
  "pos_encoding_type": "learned"
}
